{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "db = FAISS.from_texts(documents, oembed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"mistral:latest\")\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What animals are llamas related to?', 'chat_history': [HumanMessage(content='What animals are llamas related to?'), AIMessage(content=' Llamas are related to camelids, which is a family that includes vicuñas and camels.')], 'answer': ' Llamas are related to camelids, which is a family that includes vicuñas and camels.'}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What animals are llamas related to?\"\n",
    "print(conversation_chain.invoke({'question': user_question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TM®\\nМРА ЧНЫЙ МИР ОПАСНЫХ ПРИКЛЮЧЕНИЙ\\nСОДЕРЖАНИЕ\\nДОБРО ПОЖАЛОВАТЬ  \\nВ\\xa0УБЕРШРЕЙК  ������������������������������������������� 4\\nИстория  ����������������������������������������������������������������������� 5\\nДрахенфельс  ��������������������������������������������������������������� 5\\nМагнус Благочестивый  ������������������������������������������������� 6\\nОсаждённый Убершрейк  ���������������������������������������� 6\\nИмператор наносит удар  ���������������������������������������� 7\\nЛетопись Убершрейка ��������������������������������������������������� 8\\nУбершрейк сегодня  ����������������������������������������������������� 10\\nКакой сейчас год?  �������������������������������������������������� 10\\nПравители города  ������������������������������������������������������� 10\\nАльтдорфцы  ������������������������������������������������������������� 10\\nГ ородской совет  ������������������������������������������������������ 11'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader \n",
    "  \n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('./pdf_examples/Putevoditel_po_Ubershreyku.pdf') \n",
    "  \n",
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n",
    "  \n",
    "# getting a specific page from the pdf file\n",
    "raw_text = \"\\n\".join([p.extract_text() for p in reader.pages])\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_spliteer = CharacterTextSplitter(separator='\\n',\n",
    "                                        chunk_size=1024, chunk_overlap=256,\n",
    "                                        length_function=len)\n",
    "chunks = text_spliteer.split_text(raw_text)\n",
    "\n",
    "print(len(chunks))\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1940 К.\\xa0И.\\nОтравленный пир.  Великий чародей Кон -\\nстант Драхенфельс использует магию, что -\\nбы отравить на\\xa0пиру всех членов правящей \\nУбершрейком династии Брунеров. По -\\nмимо Брунеров погибают представители \\nмногих других дворянских семей Рейклан -\\nда и\\xa0 сам Император Каролус Абернауэр. \\nЮнгфройды стремительно занимают го -\\nрод и\\xa0 замок Чёрной Скалы. Немногочис -\\nленные выжившие отпрыски Дома Бруне -\\nров находят убежище в\\xa0герцогстве Вальфен.\\n2009 К.\\xa0И.\\nРейкстаг официально признаёт Юнгфрой -\\nдов новыми правителями Убершрей -\\nка. Этот декрет вызывает закономерное \\nи\\xa0вполне обоснованное негодование Валь -\\nфенов, заключивших брачный союз с\\xa0 До -\\nмом Брунеров и\\xa0посему полагающих себя \\nих\\xa0полноправными наследниками.\\n2010 К.\\xa0И.\\nВ\\xa0 результате неудачных переговоров \\nо\\xa0 разделе Убершрейка герцог Випрехт \\nфон Вальфен объявляет о\\xa0разрыве вассаль -\\nных отношений с\\xa0князьями Рейкланда.\\n2012 К.\\xa0И.\\nВойска герцога Випрехта осаждают \\nУбершрейк, и\\xa0 Юнгфройды соглашают -\\nся стать вассалами Вальфенов в\\xa0 обмен'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Как гномы повлияли на историю УБЕРШРЕЙКА?', 'chat_history': [HumanMessage(content='Как гномы повлияли на историю УБЕРШРЕЙКА?'), AIMessage(content='Гномов внутри городских стен и даже есть собственный квартал. \\n\\n(возможно, это все что можно вытащить из предоставленного текста)')], 'answer': 'Гномов внутри городских стен и даже есть собственный квартал. \\n\\n(возможно, это все что можно вытащить из предоставленного текста)'}\n"
     ]
    }
   ],
   "source": [
    "oembed = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "db = FAISS.from_texts(chunks[:70], oembed)\n",
    "model = OllamaLLM(model=\"llama3.1:8b\")\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "user_question = \"Что посторили гномы в Убершрейке?\"\n",
    "print(conversation_chain.invoke({'question': user_question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text doesn't explicitly state what the gnomes did in Uberchreak, but it mentions that \"Подкрепление, присланное гномами из карака Азгараз, разбито.\" which means \"The reinforcement sent by the gnomes from the fortress of Azgaaraz was defeated.\"\n",
      "\n",
      "So, to answer your question: The gnomes were involved in a battle or military campaign in Uberchreak, but it was unsuccessful as their reinforcements were defeated.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Что посторили гномы в Убершрейке?\"\n",
    "print(conversation_chain.invoke({'question': user_question})['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Как гномы повлияли на историю УБЕРШРЕЙКА? Ответь на английском', 'chat_history': [HumanMessage(content=' Когда началась Третья Парравонская война.?'), AIMessage(content='2308\\u200a–\\u200a2310 К.\\xa0И.\\n(Война длилась с 2308 по 2310 год после создания Империи)'), HumanMessage(content='Как гномы повлияли на историю УБЕРШРЕЙКА?'), AIMessage(content='According to the text, the gnomes:\\n\\n* Proklyadili podzemnuju kanalizaciju s mnogokratnym zadelem na budushche (translated to \"they also laid out an underground sewer system with multiple branching points for future use\").\\n* Helped rebuild Ubershreyk in 1221 K.I. (after it was destroyed by skavens).\\n* Refused to help Ubershreyk when it was being attacked by the orcs in 2303 K.I., but later agreed to help with its reconstruction.\\n* Were involved in a disagreement about the height of the wall that would be built around Ubershreyk as part of an agreement between the emperor and the king of the Azgaaroth, Zaladrin (it was supposed to be \"in thirty gnome heights\").'), HumanMessage(content='Как гномы повлияли на историю УБЕРШРЕЙКА? Ответь на английском'), AIMessage(content=\"I don't know.\")], 'answer': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Как гномы повлияли на историю УБЕРШРЕЙКА? Ответь на английском\"\n",
    "print(conversation_chain.invoke({'question': user_question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What model arcitecture do the author of the article used?', 'chat_history': [HumanMessage(content='What this article is about?'), AIMessage(content='This article appears to be a research paper discussing the use of deep convolutional neural networks (CNNs) for image classification, particularly on large-scale datasets such as ImageNet. The authors describe their architecture and methods for achieving record-breaking results on a challenging dataset using supervised learning. They also discuss the benefits of overlapping pooling in CNNs and provide results comparing different pooling schemes.\\n\\nMore specifically, this article seems to be about the development and evaluation of a deep learning model for image classification tasks, with an emphasis on scalability and performance on large datasets.'), HumanMessage(content='What model arcitecture do the author of the article used?'), AIMessage(content='The authors used an architecture with eight learned layers, consisting of five convolutional layers and three fully-connected layers. This is summarized in Figure 2. \\n\\nMore specifically:\\n\\n* The first five layers are convolutional.\\n* The remaining three layers are fully-connected.\\n* The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels.\\n* Their network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.')], 'answer': 'The authors used an architecture with eight learned layers, consisting of five convolutional layers and three fully-connected layers. This is summarized in Figure 2. \\n\\nMore specifically:\\n\\n* The first five layers are convolutional.\\n* The remaining three layers are fully-connected.\\n* The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels.\\n* Their network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.'}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What model arcitecture do the author of the article used?\"\n",
    "print(conversation_chain.invoke({'question': user_question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Response Normalization (LRN) is a technique used to reduce overfitting in neural networks, particularly in Convolutional Neural Networks (CNNs). It involves normalizing the output of neurons within a small spatial neighborhood, typically centered around each neuron. The normalization process is based on the maximum activity among neighboring neurons, and it helps to prevent any single neuron from dominating the output.\n",
      "\n",
      "In the context provided, LRN is described as being implemented in the form of lateral inhibition inspired by real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. It is applied after applying the ReLU nonlinearity in certain layers and has been shown to reduce top-1 and top-5 error rates by 1.4% and 1.2%, respectively.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Can you explain what is Local Response Normalization?\"\n",
    "print(conversation_chain.invoke({'question': user_question})['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
